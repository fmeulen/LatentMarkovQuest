# -------------------- model ----------------------------
# observe Y_i, latent U_i, observe covariates X_i
# Y_i depends on U_i
# U_i depends on U_{i-1}, X_i
# U_1 depends on Î root(X1)



struct ObservationTrajectory{S,T}
    X::Vector{S}  # vector of covariates (each element of X contains the covariates at a particular time instance)
    Y::Vector{T}  # vector of responses (each element of Y contains a K-vector of responses to the K questions)
end
#ObservationTrajectory(X,  DIM_RESPONSE) = ObservationTrajectory(X, fill(fill(1,DIM_RESPONSE), length(X)))  # constructor if only X is given

ObservationTrajectory(X, _) = ObservationTrajectory(X, fill(SA[1,1,1,1], length(X)))  # constructor if only X is given


# transition kernel of the latent chain assuming 3 latent states
#Ki(Î¸,x) = [StatsFuns.softmax([0.0, dot(x,Î¸.Î³12), -Inf])' ; StatsFuns.softmax([dot(x,Î¸.Î³21), 0.0, dot(x,Î¸.Î³23)])' ; StatsFuns.softmax([-Inf, dot(x,Î¸.Î³32), 0])']
# to avoid type instability, both Ki methods should return an element of the same type 
# slightly faster, though almost double allocation

Ki(Î¸,x,p)= SMatrix{p.NUM_HIDDENSTATES,p.NUM_HIDDENSTATES}( 
    NNlib.softmax([0.0 dot(x,Î¸.Î³12) -Inf64; dot(x,Î¸.Î³21) 0.0 dot(x,Î¸.Î³23) ; -Inf64 dot(x,Î¸.Î³32) 0.0];dims=2) ) 
    
Ki(_,::Missing,p) = SMatrix{p.NUM_HIDDENSTATES,p.NUM_HIDDENSTATES}(1.0I)
 
scaledandshifted_logistic(x) = 2.0logistic(x) - 1.0 # function that maps [0,âˆ) to [0,1)

"""
    pullback(Î¸,x,h)  
        
    returns  Ki(Î¸,x)*h
"""
function pullback(Î¸,x,h) 
    a1 = dot(StatsFuns.softmax(SA[0.0, dot(x,Î¸.Î³12), -Inf64]),h)
    a2 = dot(StatsFuns.softmax(SA[dot(x,Î¸.Î³21), 0.0 ,dot(x,Î¸.Î³23)]),h)
    a3 = dot(StatsFuns.softmax(SA[-Inf64, dot(x,Î¸.Î³32), 0.0]),h)
    SA[a1,a2,a3]
end

"""
    pullback(_, ::Missing, h)

    returns pullback in case covariates are missing
    as we assume no state change in this case, this simply returns h
"""
pullback(_, ::Missing, h) = h


mapZtoÎ»(x) = scaledandshifted_logistic.(cumsum(x))

"""
    response(Z) 
    
    make matrix [1.0-Î»[1] Î»[1]; 1.0-Î»[2] Î»[2]; 1.0-Î»[3] Î»[3]] (if 3 latent vars)

    # construct transition kernel Î› to observations
    # Î»1, Î»2, Î»3 is formed by setting Î»i = logistic(cumsum(Z)[i])
"""
function response(Z) 
        Î» = mapZtoÎ»(Z)
        SA[ one(Î»[1])-Î»[1] Î»[1];  one(Î»[2])-Î»[2] Î»[2];  one(Î»[3])-Î»[3] Î»[3]]
end

# function response(Z::Vector{T},p) where T  # slightly slower, but generic
#     Î» = mapZtoÎ»(Z)
#     v1 = SVector{p.NUM_HIDDENSTATES,T}(Î»)
#     v2 = SVector{p.NUM_HIDDENSTATES,T}(one(T) .- Î»)
#     hcat(v2, v1)
# end


Î›i(Î¸) = SA[ response(Î¸.Z1), response(Î¸.Z2), response(Î¸.Z3), response(Î¸.Z4)    ]    # assume 4 questions


#sample_observation(Î›, u) =  [sample(Weights(Î›[i][u,:])) for i in eachindex(Î›)] # sample Y | U
sample_observation(Î›, u) =  SA[sample(Weights(Î›[1][u,:])), sample(Weights(Î›[2][u,:])), sample(Weights(Î›[3][u,:])), sample(Weights(Î›[4][u,:])) ] # sample Y | U

"""
    sample(Î¸, X)             

    X: vector of covariates, say of length n
    
    samples U_1,..., U_n and Y_1,..., Y_n, where 
    U_1 ~ Î root
    for i â‰¥ 2 
        U_i | X_{i}, U_{i-1} ~ Row_{U_{i-1}} K(Î¸,X_{i})
    (thus, last element of X are not used)

"""
function sample(Î¸, X, p)            # Generate track with initial value from prior + observations
    Î› = Î›i(Î¸)
    uprev = sample(Weights(Î root(X[1],p)))                  # sample u1 (possibly depending on X[1])
    U = [uprev]
    for i in eachindex(X[2:end])
        u = sample(Weights(Ki(Î¸,X[i],p)[uprev,:]))         # Generate sample from previous state
        push!(U, copy(u))
        uprev = u
    end
    Y = [sample_observation(Î›, u) for u âˆˆ U]
    U, Y
end

"""
sample_latent(Î¸, X, U0, p)
    Î³up = rand(4)
    Î³down = rand(4)

    X = [SA[1,2,3,4], SA[4,5,6,7]]
    Î¸ = ComponentArray(Î³12=Î³up, Î³21=Î³down, Î³23=Î³up, Î³32=Î³down)
    U0 = 2
    sample_latent(Î¸, X, U0, p)
"""
function sample_latent(Î¸, X, U0, p)            # Generate track starting from U0 + for a future scenario contained in X
    uprev = U0
    U = [uprev]
    for i in eachindex(X)
        u = sample(Weights(Ki(Î¸,X[i],p)[uprev,:]))         # Generate sample from previous state
        push!(U, copy(u))
        uprev = u
    end    
    U
end




h_from_one_observation(Î›, i::Int) = Î›[:,i]

"""
    h_from_observation(Î¸, y) 

    returns message sent by observation y, when the parameter vector is Î¸
"""
function h_from_observation(Î¸, y, _) 
    Î› = Î›i(Î¸)
    h_from_one_observation(Î›[1],y[1]) .* h_from_one_observation(Î›[2],y[2]) .* h_from_one_observation(Î›[3],y[3]) .* h_from_one_observation(Î›[4],y[4])
end

# above one is twice faster and allocates half, the one below is generic
# function h_from_observation(Î¸, y, p) 
#     Î› = Î›i(Î¸)
#     a1 = [h_from_one_observation(Î›[i],y[i]) for i in eachindex(y)] 
#     SA[[prod(getindex.(a1,k)) for k in 1:p.NUM_HIDDENSTATES]...]
# end



h_from_observation(_, ::Missing,p) =  @SVector ones(p.NUM_HIDDENSTATES)


"""
    normalise!(x)

    inplace change of x to x/sum(x)
    returns log(sum(x))
"""
function normalise(x)
    s = sum(x)
    x/s, log(s)
end


"""
    loglik_and_bif(Î¸, ğ’ª::ObservationTrajectory)

    Compute loglikelihood and h-vectors from the backward information filter, for one ObservationTrajectory
"""
function loglik_and_bif(Î¸, ğ’ª::ObservationTrajectory,p)
    @unpack X, Y = ğ’ª
    N = length(Y) 
    h = h_from_observation(Î¸, Y[N], p)
    H = [h]
    loglik = zero(Î¸[1][1])
    for i in N:-1:2
        h = pullback(Î¸, X[i], h) .* h_from_observation(Î¸, Y[i-1], p)
        h, c = normalise(h)
        loglik += c
        pushfirst!(H, copy(ForwardDiff.value.(h)))
    end
    loglik += log(dot(h, Î root(X[1], p)))
    (ll=loglik, H=H)          
end

"""
    loglik(Î¸, ğ’ª::ObservationTrajectory) 

    Returns loglikelihood at Î¸ for one ObservationTrajectory
"""    
function loglik(Î¸, ğ’ª::ObservationTrajectory, p) 
    @unpack X, Y = ğ’ª
    N = length(Y) 
    h = h_from_observation(Î¸, Y[N], p)
    loglik = zero(Î¸[1][1])
    for i in N:-1:2
        h = pullback(Î¸, X[i], h) .* h_from_observation(Î¸, Y[i-1], p)
        h, c = normalise(h)
        loglik += c
    end
    loglik + log(dot(h, Î root(X[1],p)))
end


"""
    loglik(Î¸, ğ’ªs::Vector)

    Returns loglikelihood at Î¸ for multplies ObservationTrajectories
"""    
function loglik(Î¸, ğ’ªs::Vector, p)
    ll = zero(Î¸[1][1])
    for i âˆˆ eachindex(ğ’ªs)
        ll += loglik(Î¸, ğ’ªs[i], p)
    end
    ll 
end

loglik(ğ’ª, p) = (Î¸) -> loglik(Î¸, ğ’ª, p) 

âˆ‡loglik(ğ’ª, p) = (Î¸) -> ForwardDiff.gradient(loglik(ğ’ª, p), Î¸)

# check
function sample_guided(Î¸, ğ’ª, H, p)# Generate approximate track
    X = ğ’ª.X
    N = length(H) # check -1?
    uprev = sample(Weights(Î root(X[1], p) .* H[1])) # Weighted prior distribution
    uáµ’ = [uprev]
    for i=2:N
        w = Ki(Î¸,X[i], p)[uprev,:] .* H[i]         # Weighted transition density
        u = sample(Weights(w))
        push!(uáµ’, u)
        uprev = u
    end
    uáµ’
end

function unitvec(k,K)
    ee = zeros(K); 
    ee[k] = 1.0
    SVector{K}(ee)
end

function viterbi(Î¸, ğ’ª::ObservationTrajectory, p) 
    @unpack NUM_HIDDENSTATES = p
    @unpack X, Y = ğ’ª
    N = length(Y) 
    h = h_from_observation(Î¸, Y[N], p)
    mls = [argmax(h)]  # m(ost) l(ikely) s(tate)
    h = unitvec(mls[1], NUM_HIDDENSTATES)
    #loglik = zero(Î¸[1][1])
    for i in N:-1:2
        h = pullback(Î¸, X[i], h) .* h_from_observation(Î¸, Y[i-1], p)
        pushfirst!(mls, argmax(h))
        h = unitvec(mls[1], NUM_HIDDENSTATES)
    end
    mls
end


############ use of Turing to sample from the posterior ################

abstract type Ztype end
struct Restricted <: Ztype end  # same Î» for all questions
struct Unrestricted <: Ztype end # # separate Î» for all questions



# model with Î»vector the same for all questions (less parameters)
@model function logtarget(::Restricted, ğ’ªs, p)
    Î³up ~ filldist(Normal(0,5), p.DIM_COVARIATES)
    Î³down ~ filldist(Normal(0,5), p.DIM_COVARIATES)  
    Z0 ~ filldist(Exponential(), p.NUM_HIDDENSTATES) 
    Turing.@addlogprob! loglik(ComponentArray(Î³12 = Î³up, Î³21 = Î³down, Î³23 = Î³up, Î³32 = Î³down, Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0), ğ’ªs, p)
end

@model function logtarget(::Unrestricted, ğ’ªs, p)
    Î³up ~ filldist(Normal(0,5), p.DIM_COVARIATES)
    Î³down ~ filldist(Normal(0,5), p.DIM_COVARIATES)  
    
    Z1 ~ filldist(Exponential(), p.NUM_HIDDENSTATES) 
    Z2 ~ filldist(Exponential(), p.NUM_HIDDENSTATES) 
    Z3 ~ filldist(Exponential(), p.NUM_HIDDENSTATES) 
    Z4 ~ filldist(Exponential(), p.NUM_HIDDENSTATES) 
    Turing.@addlogprob! loglik(ComponentArray(Î³12 = Î³up, Î³21 = Î³down, Î³23 = Î³up, Î³32 = Î³down, Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4), ğ’ªs, p)
end

# now with different gammas
# @model function logtarget_large(ğ’ªs, p)
#     Î³12 ~ filldist(Normal(0,5), p.DIM_COVARIATES)#MvNormal(fill(0.0, 2), 2.0 * I)
#     Î³13 ~ filldist(Normal(0,5), p.DIM_COVARIATES)#MvNormal(fill(0.0, 2), 2.0 * I)
#     Î³21 ~ filldist(Normal(0,5), p.DIM_COVARIATES)  #MvNormal(fill(0.0, 2), 2.0 * I)
#     Î³23 ~ filldist(Normal(0,5), p.DIM_COVARIATES)  #MvNormal(fill(0.0, 2), 2.0 * I)
#     Î³31 ~ filldist(Normal(0,5), p.DIM_COVARIATES)  #MvNormal(fill(0.0, 2), 2.0 * I)
#     Î³32 ~ filldist(Normal(0,5), p.DIM_COVARIATES)  #MvNormal(fill(0.0, 2), 2.0 * I)

#     Z1 ~ filldist(Exponential(), p.NUM_HIDDENSTATES) 
#     Z2 ~ filldist(Exponential(), p.NUM_HIDDENSTATES) 
#     Z3 ~ filldist(Exponential(), p.NUM_HIDDENSTATES) 
#     Z4 ~ filldist(Exponential(), p.NUM_HIDDENSTATES) 
#     Turing.@addlogprob! loglik(ComponentArray(Î³12 = Î³12, Î³13 = Î³13, Î³21 = Î³21, Î³23 = Î³23, Î³31 = Î³31, Î³32 = Î³32, Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4), ğ’ªs, p)
# end


